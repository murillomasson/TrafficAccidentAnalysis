{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35c34a99-cec1-4f5a-9a91-9465602a0818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace, year, substring, to_date, regexp_extract, asc, desc, split, when, count, trim\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from dotenv import load_dotenv\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import shapely\n",
    "import calendar\n",
    "import requests\n",
    "import locale\n",
    "import boto3\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d234d14-13d2-4310-b2dd-a3555053c05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': 1, 'data_extracao': '2023-05-01T01:36:51', 'longitude': -51.178292, 'latitude': -30.066858, 'idacidente': 643556, 'data': '2018-01-02T00:00:00', 'hora': '18:00:00.0000000', 'idade': 37, 'sexo': 'MASCULINO', 'sit_vitima': 'CONDUTOR', 'log1': 'AV ROCIO', 'log2': 'R TEN ALPOIM', 'predial1': 0, 'regiao': 'LESTE', 'tipo_acid': 'ABALROAMENTO', 'auto': 0, 'taxi': 1, 'onibus_urb': 0, 'onibus_met': 0, 'onibus_int': 0, 'caminhao': 0, 'moto': 1, 'carroca': 0, 'bicicleta': 0, 'outro': 0, 'lotacao': 0, 'dia_sem': 'TERÇA-FEIRA', 'periododia': 'NOITE', 'fx_et': '36 A 45', 'tipo_veic': 'MOTOCICLETA', 'consorcio': ''}\n",
      "Number of records: 29482\n"
     ]
    }
   ],
   "source": [
    "URL = \"https://dadosabertos.poa.br/api/3/action/datastore_search?resource_id=a46aaaca-8cc1-4082-aa78-ce9f859e2df5&limit=30000\"\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# AWS CONFIGS\n",
    "ACCESS_KEY_ID = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "SECRET_ACCESS_KEY = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "REGION_NAME = os.getenv('AWS_REGION')\n",
    "BUCKET_NAME = 'traffic-accident-porto-alegre'\n",
    "\n",
    "# PYSPARK CONFIGS\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Traffic Accident Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data = requests.get(URL).json()\n",
    "records = data[\"result\"][\"records\"]\n",
    "\n",
    "# Check if the data is retrieved correctly from URL\n",
    "try:\n",
    "    if records:\n",
    "        print(\"- Endpoint connected! A SAMPLE FROM THE ENDPOINT: \", records[0])\n",
    "    else:\n",
    "        print(\"- No records found.\")\n",
    "\n",
    "    print(\"- Number of records:\", len(records))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"- An error occurred:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bf36a90-d954-4907-beb5-b8d24d0248b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "records in the JSON data:  29482\n",
      "records in the DataFrame:  29482\n",
      "+---+-------------------+----------------+-----+---------+----------+--------------------+------+-------------+----+----+----------+----------+----------+--------+----+-------+---------+-----+-------+-------------+----------+-------+\n",
      "|_id|               data|            hora|idade|     sexo|sit_vitima|                log1|regiao|    tipo_acid|auto|taxi|onibus_urb|onibus_met|onibus_int|caminhao|moto|carroca|bicicleta|outro|lotacao|      dia_sem|periododia|  fx_et|\n",
      "+---+-------------------+----------------+-----+---------+----------+--------------------+------+-------------+----+----+----------+----------+----------+--------+----+-------+---------+-----+-------+-------------+----------+-------+\n",
      "|  4|2018-01-05T00:00:00|18:00:00.0000000|   34|MASCULINO|  CONDUTOR|           R REGENTE| LESTE|      COLISÃO|   1|   0|         0|         0|         0|       0|   1|      0|        0|    0|      0|  SEXTA-FEIRA|     NOITE|26 A 35|\n",
      "| 10|2018-01-10T00:00:00|16:10:00.0000000|   63| FEMININO|  CONDUTOR|         AV SERTORIO| NORTE|      COLISÃO|   1|   0|         0|         0|         0|       1|   0|      0|        0|    0|      0| QUARTA-FEIRA|     TARDE|   60 +|\n",
      "| 43|2018-01-17T00:00:00|21:00:00.0000000|   29|MASCULINO|  CONDUTOR|AV BORGES DE MEDE...|CENTRO| ABALROAMENTO|   1|   0|         0|         0|         0|       0|   0|      0|        1|    0|      0| QUARTA-FEIRA|     NOITE|26 A 35|\n",
      "| 49|2018-01-22T00:00:00|22:30:00.0000000|   31| FEMININO|  OCUPANTE|  AV BENTO GONCALVES| LESTE|      COLISÃO|   1|   0|         0|         0|         1|       0|   0|      0|        0|    0|      0|SEGUNDA-FEIRA|     NOITE|26 A 35|\n",
      "| 50|2018-01-23T00:00:00|10:30:00.0000000|   71| FEMININO|  PEDESTRE|   AV PROTASIO ALVES| LESTE|ATROPELAMENTO|   0|   0|         0|         0|         0|       0|   1|      0|        0|    0|      0|  TERÇA-FEIRA|     MANHÃ|   60 +|\n",
      "+---+-------------------+----------------+-----+---------+----------+--------------------+------+-------------+----+----+----------+----------+----------+--------+----+-------+---------+-----+-------+-------------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Defining the DataFrame schema\n",
    "    schema = StructType([\n",
    "        StructField(\"_id\", IntegerType()),\n",
    "        StructField(\"data\", StringType()),\n",
    "        StructField(\"hora\", StringType()),\n",
    "        StructField(\"idade\", IntegerType()),\n",
    "        StructField(\"sexo\", StringType()),\n",
    "        StructField(\"sit_vitima\", StringType()),\n",
    "        StructField(\"log1\", StringType()),\n",
    "        StructField(\"regiao\", StringType()),\n",
    "        StructField(\"tipo_acid\", StringType()),\n",
    "        StructField(\"auto\", IntegerType()),\n",
    "        StructField(\"taxi\", IntegerType()),\n",
    "        StructField(\"onibus_urb\", IntegerType()),\n",
    "        StructField(\"onibus_met\", IntegerType()),\n",
    "        StructField(\"onibus_int\", IntegerType()),\n",
    "        StructField(\"caminhao\", IntegerType()),\n",
    "        StructField(\"moto\", IntegerType()),\n",
    "        StructField(\"carroca\", IntegerType()),\n",
    "        StructField(\"bicicleta\", IntegerType()),\n",
    "        StructField(\"outro\", IntegerType()),\n",
    "        StructField(\"lotacao\", IntegerType()),\n",
    "        StructField(\"dia_sem\", StringType()),\n",
    "        StructField(\"periododia\", StringType()),\n",
    "        StructField(\"fx_et\", StringType()),\n",
    "    ])\n",
    "\n",
    "    data_tuples = [\n",
    "        (\n",
    "            record[\"_id\"], record[\"data\"], record[\"hora\"], record[\"idade\"], record[\"sexo\"],\n",
    "            record[\"sit_vitima\"], record[\"log1\"], record[\"regiao\"], record[\"tipo_acid\"], record[\"auto\"],\n",
    "            record[\"taxi\"], record[\"onibus_urb\"], record[\"onibus_met\"], record[\"onibus_int\"], record[\"caminhao\"],\n",
    "            record[\"moto\"], record[\"carroca\"], record[\"bicicleta\"], record[\"outro\"], record[\"lotacao\"], record[\"dia_sem\"],\n",
    "            record[\"periododia\"], record[\"fx_et\"]\n",
    "        ) for record in records\n",
    "    ]\n",
    "    \n",
    "    # Create the DataFrame with the updated schema and data_tuples\n",
    "    df = spark.createDataFrame(data_tuples, schema)\n",
    "\n",
    "    # Check if both the numbers in the JSON data and DataFrame are the same\n",
    "    print(\"records in the JSON data: \", len(records))\n",
    "    print(\"records in the DataFrame: \", df.count())\n",
    "    print(\"A RANDOM SAMPLE FROM DATAFRAME:\")\n",
    "    df.sample(False, 0.1).show(5)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40af3909-bb8c-4071-8172-5afc0595a784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- First 10 digits extracted from the 'data' column.\n",
      "- Eight digits extracted from the 'hora' column for the majority of records.\n",
      "- Leading and trailing whitespaces removed from the 'log1' column.\n",
      "- The name of the first column corresponds to 'id'.\n",
      "- The 'log_cleaned' column has been successfully created.\n",
      "- Leading and trailing whitespaces removed from the 'log_cleaned' column.\n",
      "- Pattern '^(DOS |DAS |DA |DO |TL |DESEMBARGADOR )' has been successfully applied to 'log_cleaned' column.\n",
      "- Pattern '^(NS DA CONCEICAO-AC AV )' has been successfully applied to 'log_cleaned' column.\n",
      "- Pattern '^(QUATRO JARDIM )' has been successfully applied to 'log_cleaned' column.\n",
      "- Pattern '^(NS DA CONCEICAO-AC AV )' has been successfully applied to 'log_cleaned' column.\n",
      "- Pattern '^ESTRELA$' has been successfully applied to 'log_cleaned' column.\n",
      "- Pattern '^ANDRE DA ROCHA$' has been successfully applied to 'log_cleaned' column.\n",
      "- Pattern '^SEVERO DULLI$' has been successfully applied to 'log_cleaned' column.\n",
      "- Pattern '^A I VILA BOM JESUS$' has been successfully applied to 'log_cleaned' column.\n",
      "- Pattern '^MORENO LOUREIRO LIMA$' has been successfully applied to 'log_cleaned' column.\n",
      "- Pattern '^A VILA N S FATIMA-DIVINEIA$' has been successfully applied to 'log_cleaned' column.\n",
      "! ERROR: Some undesired values still remain in the DataFrame. COUNT:  4\n",
      "_semple of remaining undesired values:_\n",
      "ALBERTO BINS\n",
      "LUZ\n",
      "7137\n",
      "VA NOVE CEFER UM\n"
     ]
    }
   ],
   "source": [
    "# ___DATA CLEANING\n",
    "class DataCleaning:\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "    # This function was commented out as it was determined that there was no need to transform null spaces into 'N/A'.\n",
    "    '''def fillna_values(self):\n",
    "        columns_to_exclude = [\"idade\", \"log1\"]\n",
    "        for column in self.df.columns:\n",
    "            if column not in columns_to_exclude:\n",
    "                self.df = self.df.withColumn(column, when(col(column).isNull() | (col(column) == \"\"), \"N/A\").otherwise(col(column)))'''\n",
    "\n",
    "    def change_id_field_name(self):\n",
    "        self.df = self.df.withColumnRenamed(\"_id\", \"id\")\n",
    "\n",
    "        \n",
    "    def extract_day_from_date(self):\n",
    "        self.df = self.df.withColumn('data', F.to_date(F.substring(F.col('data'), 1, 10)))\n",
    "\n",
    "        \n",
    "    def extract_first_8_digits_from_hora(self):\n",
    "        self.df = self.df.withColumn('hora', F.substring(F.col('hora'), 1, 8))\n",
    "\n",
    "        \n",
    "    def remove_whitespaces_from_log1(self):\n",
    "        self.df = self.df.withColumn('log1', F.trim(F.col('log1')))\n",
    "\n",
    "        \n",
    "    def replace_log1_values(self):\n",
    "        self.df = self.df.withColumn('log1', regexp_replace(col('log1'), r'^(\\bPROTASIO ALVES\\b)', 'AV PROTASIO ALVES'))\n",
    "        self.df = self.df.withColumn('log1', regexp_replace(col('log1'), r'^(\\R A AV ASSIS BRASIL\\b)', 'AV ASSIS BRASIL'))\n",
    "\n",
    "        \n",
    "    def remove_decimal_age_value(self):\n",
    "        self.df = self.df.withColumn('idade', regexp_replace(col('idade'), r\"\\.0$\", \"\"))\n",
    "\n",
    "      \n",
    "    def create_new_column_log_correspondency(self):\n",
    "        self.df = self.df.withColumn('log_cleaned', regexp_replace(col('log1'), '^(R |AV |ESTR |TUN |BC |TRAV |AL |LG |PSG |PCA |VDT |PRQ |AC |DOS |DAS |DO |DA )\\\\s*', ''))\n",
    "        self.df = self.df.withColumn('log_cleaned', regexp_replace(col('log_cleaned'), '^(DOS |DAS |DA |DO |TL |DESEMBARGADOR )', ''))\n",
    "        self.df = self.df.withColumn('log_cleaned', regexp_replace(col('log_cleaned'), '^(NS DA CONCEICAO-AC AV )', ''))\n",
    "        self.df = self.df.withColumn('log_cleaned', regexp_replace(col('log_cleaned'), '^(QUATRO JARDIM )', ''))\n",
    "        self.df = self.df.withColumn('log_cleaned', regexp_replace(col('log_cleaned'), '^(NS DA CONCEICAO-AC AV )', ''))\n",
    "        self.df = self.df.withColumn('log_cleaned', regexp_replace(col('log_cleaned'), '^ESTRELA$', 'ESTRELA PARQUE BELEM'))\n",
    "        self.df = self.df.withColumn('log_cleaned', regexp_replace(col('log_cleaned'), '^ANDRE DA ROCHA$', 'DES ANDRE DA ROCHA'))\n",
    "        self.df = self.df.withColumn('log_cleaned', regexp_replace(col('log_cleaned'), '^SEVERO DULLI$', 'SEVERO DULLIUS'))\n",
    "        self.df = self.df.withColumn('log_cleaned', regexp_replace(col('log_cleaned'), '^A I VILA BOM JESUS$', 'A E VILA BOM JESUS'))\n",
    "        self.df = self.df.withColumn('log_cleaned', regexp_replace(col('log_cleaned'), '^A VILA N S FATIMA-DIVINEIA$', 'C VILA N S FATIMA-DIVINEIA'))\n",
    "        self.df = self.df.withColumn('log_cleaned', regexp_replace(col('log_cleaned'), '^HUGO CANDAL$', 'DES HUGO CANDAL'))\n",
    "\n",
    "    def remove_whitespaces_from_log_cleaned(self):\n",
    "        self.df = self.df.withColumn('log_cleaned', F.trim(F.col('log_cleaned')))       \n",
    "        \n",
    "    def remove_lines_with_no_log_correspondency(self):\n",
    "        lst = ['LEGALIDADE E DA DEMOCRACIA', '', 'RTL CARLOS GOMES-PLINIO B MILANO',\n",
    "             'LE MERCADO PUBLICO CENTRAL-GALERIA', 'HUGO CANDAL', 'MORENO LOUREIRO LIMA',\n",
    "             'OTILIA K DE ARAUJO-VIAMAO', 'DIR QUATRO MIL NOVECENTOS',\n",
    "             'nossa sra aparecida', 'ALBERTO  BINS', 'TRES MIL CINQUENTA TRES', 'LUZ'\n",
    "             '7137' 'VA NOVE CEFER UM', 'DOIS TEN ARY TARRAGO-PAULO M COELHO',\n",
    "             'RP E UM VILA NOVA SANTA ROSA', 'MORENO LOUREIRO LIMA']\n",
    "        \n",
    "        self.df = self.df.filter(~col('log_cleaned').isin(lst))\n",
    "       \n",
    "    def create_pyspark_dataframe_csv(self):    \n",
    "        df_pandas = self.df.toPandas()\n",
    "        df_pandas.to_csv('cl_data.csv', index=False)\n",
    "\n",
    "          \n",
    "    def create_pd_geo_dataframe_csv_and_compare(self):\n",
    "        gdf = gpd.read_file(\"EixosLogradouros/EixosLogradouros.shp\")\n",
    "\n",
    "        def get_wkt(geom):\n",
    "            if geom is not None and isinstance(geom, shapely.geometry.base.BaseGeometry):\n",
    "                return geom.wkt\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        df_pandas_geo = gdf.drop('geometry', axis=1).copy()\n",
    "        df_pandas_geo['geometry'] = gdf['geometry'].apply(get_wkt)  \n",
    "        \n",
    "        \n",
    "    # ___TESTS    \n",
    "    # This function was commented out as it was determined that there was no need to transform null spaces into 'N/A'.\n",
    "    ''' def check_columns_replaced(self):\n",
    "        excluded_columns = [\"idade\", \"log1\"]\n",
    "        for column in self.df.columns:\n",
    "            if column not in excluded_columns:\n",
    "                test = self.df.filter(F.col(column).isNull() | (F.col(column) == \"\")).count()\n",
    "                replaced_values = self.df.select(column).rdd.flatMap(lambda x: x).collect()\n",
    "                if test == 0 or 'N/A' not in replaced_values:\n",
    "                    print(f\"- Column '{column}' has been replaced correctly with 'N/A' for null or empty string values.\")\n",
    "                else:\n",
    "                    print(f\"- ERROR: Some values in column '{column}' are not replaced correctly with 'N/A' for null or empty string values.\")'''\n",
    "    \n",
    "    \n",
    "    def check_first_column_name(self):\n",
    "        if self.df.columns[0] == \"id\":\n",
    "            print(\"- The name of the first column corresponds to 'id'.\")\n",
    "        else:\n",
    "            print(\"- The name of the first column does not correspond to 'id'.\")\n",
    "    \n",
    "    \n",
    "    def check_date_extraction(self):\n",
    "        if self.df.filter((F.length(F.col('data')) != 10) | (F.col('data') == \" \")).count() == 0:\n",
    "            print(\"- First 10 digits extracted from the 'data' column.\")\n",
    "        else:\n",
    "            print(\"! ERROR: 'data' is not extracted correctly. COUNTER:\", self.df.filter((F.length(F.col('data')) != 10) | (F.col('data') == \" \")).count())\n",
    "            print(\"- Sample of 5 random records where 'data' is not extracted correctly:\")\n",
    "            self.df.filter((F.length(F.col('data')) != 10) | (F.col('data') == \"N/A\")).sample(False, 0.1).show(5, truncate=False)\n",
    "\n",
    "            \n",
    "    def check_hour_extraction(self):\n",
    "        eight_digits_count = self.df.filter(F.length(F.col('hora')) == 8).count()\n",
    "        null_count = self.df.filter(F.col('hora').isNull()).count()\n",
    "\n",
    "        if eight_digits_count > 0:\n",
    "            print(\"- Eight digits extracted from the 'hora' column for the majority of records.\")\n",
    "        else:\n",
    "            print(\"! ERROR: No eight-digit values found in the 'hora' column.\")\n",
    "\n",
    "        if null_count > 0:\n",
    "            print(\"- ALERT: Some values in the 'hora' column are counted as null. COUNTER: \", null_count)\n",
    "            print(\"- Sample of 5 random records where 'hora' is not extracted correctly:\")\n",
    "            self.df.filter((F.length(F.col('hora')) != 8) | (F.col('hora') == \" \")).sample(False, 0.1).show(5, truncate=False)\n",
    "\n",
    "\n",
    "            \n",
    "    def check_whitespaces_removed_log1(self):\n",
    "        if self.df.filter(F.col('log1').rlike(r'^\\s+|\\s+$')).count() == 0:\n",
    "            print(\"- Leading and trailing whitespaces removed from the 'log1' column.\")\n",
    "        else:\n",
    "            print(\"! ERROR: Leading and trailing whitespaces are not removed correctly. COUNTER: \", self.df.filter(F.col('log1').rlike(r'^\\s+|\\s+$')).count())\n",
    "           \n",
    "        \n",
    "    def check_new_column_log_cleaned(self):\n",
    "        if 'log_cleaned' in self.df.columns:\n",
    "            print(\"- The 'log_cleaned' column has been successfully created.\")\n",
    "        else:\n",
    "            print(\"! ERROR: The 'log_cleaned' column has not been created yet.\")\n",
    "          \n",
    "            \n",
    "    def check_log_cleaned_alterations(self):\n",
    "        regex_patterns = [\n",
    "            '^(DOS |DAS |DA |DO |TL |DESEMBARGADOR )',\n",
    "            '^(NS DA CONCEICAO-AC AV )',\n",
    "            '^(QUATRO JARDIM )',\n",
    "            '^(NS DA CONCEICAO-AC AV )',\n",
    "            '^ESTRELA$',\n",
    "            '^ANDRE DA ROCHA$',\n",
    "            '^SEVERO DULLI$',\n",
    "            '^A I VILA BOM JESUS$',\n",
    "            '^MORENO LOUREIRO LIMA$',\n",
    "            '^A VILA N S FATIMA-DIVINEIA$'\n",
    "        ]\n",
    "\n",
    "        for pattern in regex_patterns:\n",
    "            test_count = self.df.filter(col('log_cleaned').rlike(pattern)).count()\n",
    "            if test_count == 0:\n",
    "                print(f\"- Pattern '{pattern}' has been successfully applied to 'log_cleaned' column.\")\n",
    "            else:\n",
    "                print(f\"! ERROR: Pattern '{pattern}' has not been applied correctly to 'log_cleaned' column.\")\n",
    "    \n",
    "    def check_whitespaces_removed_log_cleaned(self):\n",
    "        if self.df.filter(F.col('log_cleaned').rlike(r'^\\s+|\\s+$')).count() == 0:\n",
    "            print(\"- Leading and trailing whitespaces removed from the 'log_cleaned' column.\")\n",
    "        else:\n",
    "            print(\"! ERROR: Leading and trailing whitespaces are not removed correctly. COUNTER: \", self.df.filter(F.col('log_cleaned').rlike(r'^\\s+|\\s+$')).count())      \n",
    "        \n",
    "    def check_log_removed(self):\n",
    "        lst = ['LEGALIDADE E DA DEMOCRACIA', 'DA LEGALIDADE E DA DEMOCRACIA', '', 'RTL CARLOS GOMES-PLINIO B MILANO',\n",
    "               'LE MERCADO PUBLICO CENTRAL-GALERIA', 'HUGO CANDAL', 'MORENO LOUREIRO LIMA',\n",
    "               'OTILIA K DE ARAUJO-VIAMAO', 'DIR QUATRO MIL NOVECENTOS',\n",
    "               'nossa sra aparecida', 'ALBERTO BINS', 'TRES MIL CINQUENTA TRES', 'LUZ',\n",
    "               '7137', 'VA NOVE CEFER UM', 'DOIS TEN ARY TARRAGO-PAULO M COELHO',\n",
    "               'RP E UM VILA NOVA SANTA ROSA']\n",
    "\n",
    "        filtered_df = self.df.filter(~col('log_cleaned').isin(lst))\n",
    "        if filtered_df.count() == self.df.count():\n",
    "            print(\"- All rows have been removed correctly.\")\n",
    "        else:\n",
    "            remaining_values_df = self.df.select('log_cleaned').subtract(filtered_df.select('log_cleaned'))\n",
    "            remaining_count = remaining_values_df.count()\n",
    "            print(f\"ALERT: >{remaining_count}< undesired values still ramain on DataFrame.\")\n",
    "            print(\"These values can only conflict if there is a connection with the file for mapping accidents by address in the city of Porto Alegre: \")\n",
    "\n",
    "            if remaining_count > 0:\n",
    "                print(\"_semple of remaining undesired values:_\")\n",
    "                remaining_values = remaining_values_df.distinct().collect()\n",
    "                \n",
    "                for row in remaining_values:\n",
    "                    print(row.log_cleaned)\n",
    "         \n",
    "\n",
    "    def execute_cleaning(self):\n",
    "        self.extract_day_from_date()\n",
    "        self.change_id_field_name()\n",
    "        self.extract_first_8_digits_from_hora()\n",
    "        self.remove_whitespaces_from_log1()\n",
    "        self.replace_log1_values()\n",
    "        self.remove_decimal_age_value()\n",
    "        self.create_new_column_log_correspondency()\n",
    "        self.remove_whitespaces_from_log_cleaned()\n",
    "        self.remove_lines_with_no_log_correspondency()\n",
    "        self.create_pyspark_dataframe_csv()\n",
    "        self.create_pd_geo_dataframe_csv_and_compare()\n",
    " \n",
    "\n",
    "    def run_tests(self):\n",
    "        self.check_date_extraction()\n",
    "        self.check_hour_extraction()\n",
    "        self.check_whitespaces_removed_log1()\n",
    "        self.check_first_column_name()\n",
    "        self.check_new_column_log_cleaned()\n",
    "        self.check_whitespaces_removed_log_cleaned()\n",
    "        self.check_log_cleaned_alterations()\n",
    "        self.check_log_removed()\n",
    "        \n",
    "        \n",
    "    def run_data_cleaning(self):\n",
    "        try:\n",
    "            self.execute_cleaning()\n",
    "            self.run_tests()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"!!! Error occurred during data cleaning:\", str(e))\n",
    "\n",
    "dc = DataCleaning(df)\n",
    "\n",
    "dc.run_data_cleaning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f5d4431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File successfully uploaded to AWS S3.\n"
     ]
    }
   ],
   "source": [
    "# REMEMBER TO PRECONFIGURE THE AWS KEYS BEFORE EXECUTE THIS CELL\n",
    "\n",
    "s3_client = boto3.client('s3',\n",
    "                        aws_access_key_id=ACCESS_KEY_ID,\n",
    "                        aws_secret_access_key=SECRET_ACCESS_KEY,\n",
    "                        region_name=REGION_NAME\n",
    "                        )\n",
    "\n",
    "local_file_path = 'cl_data.csv'\n",
    "s3_file_name = 'data.csv'\n",
    "\n",
    "s3_client.upload_file(local_file_path, BUCKET_NAME, s3_file_name)\n",
    "\n",
    "print(\"- File successfully uploaded to AWS S3.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
